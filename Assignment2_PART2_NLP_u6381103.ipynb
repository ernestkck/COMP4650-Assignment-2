{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Part 1 (ML) [7 pts]\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### In sections 1.2-1.5 of the Machine Learning notebook, there are tasks for you to complete. Be sure to submit BOTH the Machine Learning demo notebook and this notebook.\n",
    "</spaN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Part 2 (NLP) [8 pts]\n",
    "\n",
    "### 2.1 Fast Text [3 pts]\n",
    "\n",
    "FastText[1] is a neural network based text classification model designed to be computationally efficient. Your task is to implement the FastText algorithm by completeing the code in the following cells. You will need to read through the provided fastText.pdf paper, which explains the algorithm. You do not need to implement Hierarchical softmax (2.1) or N-gram features (2.2), you only need to implement the basic architecture described in (2). \n",
    "\n",
    "The FastText model will be trained using mini-batch gradient descent. When the training data are sequences of variable lengths we can not simply stack multiple training sequences into one tensor. Instead, it is common to assume that there is a maximal sequence length, so that all sequences in a batch are fitted into tensors of the same dimensions. For sequences shorter than the maximal length, we append them with a special pad word so that all sequences in a batch are of the same length. A *pad word* is a special token, whose embedding is an all-zero vector, so that the presence of pad words does not change the output of the model. In this code, the pad word has an ID of 0, when implementing your embeddings you should ensure that this ID is always embedded to a vector of all zeros. Additionally, you will need to know how many words are in each input sentence (before they got padded to be the same length), this is provided as an input parameter to your FastText model.\n",
    "\n",
    "[1] Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas. Bag of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759., 2016. [INCLUDED AS PART OF ASSIGNMENT 2 .ZIP PACKAGE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ernes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys, getopt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "### You need to complete the foward() and __init__() functions below [3 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \"\"\"Define the computation graph for fasttext model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim, learning_rate):\n",
    "        \"\"\"Init the model with default parameters/hyperparameters.\"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_func = F.cross_entropy\n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.W = torch.nn.Linear(embedding_dim, num_classes) \n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x, sens_lengths):\n",
    "        x = self.embedder(x).sum(axis=1).div(sens_lengths.float())\n",
    "        x = self.W(x.float())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ernes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences is 8216. \n",
      "PAD word id is 0 .\n",
      "Unknown word id is 1 .\n",
      "size of vocabulary is 3666. \n",
      "read 1000 sentences from question_2-1_data\\sentences_train.txt .\n",
      "read 500 sentences from question_2-1_data\\sentences_dev.txt .\n",
      "read 500 sentences from question_2-1_data\\sentences_test.txt .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FastText. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : train loss = 1.1260620665550232 , validation accuracy = 0.3019999861717224 .\n",
      "Epoch 1 : train loss = 1.1212979578971862 , validation accuracy = 0.32199999690055847 .\n",
      "Epoch 2 : train loss = 1.117397426366806 , validation accuracy = 0.3540000021457672 .\n",
      "Accuracy on the test set : 0.328000009059906.\n"
     ]
    }
   ],
   "source": [
    "from fasttext import load_question_2_1, train_fast_text\n",
    "\n",
    "word_to_id, train_data, valid_data, test_data = load_question_2_1('question_2-1_data')\n",
    "model = FastText(len(word_to_id)+2, num_classes, embedding_dim=embedding_dim, learning_rate=learning_rate)\n",
    "\n",
    "model_file_path = os.path.join('models', 'fasttext_model_file_q2-1')\n",
    "train_fast_text(model, train_data, valid_data, test_data, model_file_path, batch_size=batch_size, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Question Classification [3 pts]\n",
    "\n",
    "Understanding questions is a key problem in chatbots and question answering systems. In the open-domain setting, it is difficult to find right answers in the huge search space. To tackle the problem, one approach is to categorize questions into a finite set of semantic classes, and each semantic class corresponds to a small answer space.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Your task is to implement a question classification model in Pytorch, and apply it to the question_2_2_data provided in this assignment.\n",
    "</span>\n",
    "\n",
    "Notes: \n",
    "\n",
    "\n",
    "-  Please do NOT submit your data directories, pretrained word embeddings, and Pytorch library!\n",
    "\n",
    "-  You may consider reusing parts of the code above\n",
    "\n",
    "-  Code must be submitted with the assignment for purposes of plagiarism detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset provided contains three files: **train.json**, **validation.json**, and **test.json**, which are the training dataset, validation dataset, and the test dataset, respectively. \n",
    "See an example below: \n",
    "```\n",
    "{\n",
    "   \"ID\": S1,\n",
    "   \"Label\": 3,\n",
    "   \"Sentence\":\"What country has the best defensive position in the board game Diplomacy ?\"\n",
    "}\n",
    "```\n",
    "In the training set and the validation set, the response variable is called `Label`. Your task is to predict the `Label` for each sentence in the test set. \n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The performance of your prediction will be evaluated automatically on Kaggle using __Accuracy__ , which is defined as the number of correct predictions divided by the total number of sentences in the test set (https://classeval.wordpress.com/introduction/basic-evaluation-measures/).\n",
    "\n",
    "It is important to understand that the leaderboard score will be only computed based on the half of the test cases, and the remaining half will be computed after the deadline based on your selected submission. This process will ensure that your performance is not only applicable for the known test cases, but also generalised to the unknown test cases. We will combine these two performances to score the first assignment.\n",
    "\n",
    "Your score will be computed using a lower bound and an upper bound, which will be shown on the Kaggle leader board. \n",
    "Achieving an accuracy equal and below the lower bound amounts to a grade of zero, while achieving the upper bound amounts to the full points (here 3 points, see score distribution here below).\n",
    "Consequently, your score for this competition task will be calculated based on:\n",
    "\n",
    "$$\n",
    "    \\operatorname{Your\\_Score} = \\frac{Your\\_Accuracy - Lower\\_Bound}{Upper\\_Bound - Lower\\_Bound} * 3\n",
    "$$\n",
    "Notes about the lower bound and upper bounds predictors:\n",
    "\n",
    "* The **lower bound** is the performance obtained by a classifer that always picks the majority class according to the class distribution in the training set.\n",
    "* The **upper bound** is generated by an \"in-house\" classifier trained on the same dataset that you were given.\n",
    "\n",
    "There are many possibilities to achieve better results than this. However, the **only** labeled training dataset to train your model should be the provided **train.json**. \n",
    "If you obtain a better performance than the upper bound, then you will have a grade higher than 3 points for this question. This can be useful to compensate for any lost points for the whole assignment.\n",
    "However, the total mark of this assignment is capped at 10 marks.\n",
    "\n",
    "### Kaggle competition\n",
    "\n",
    "- You will be given a link to join the competition during your labs.\n",
    "- Before submitting the result, first go to **team** menu and change your **team name** as **your university id**.\n",
    "- You need to upload the generated result file to Kaggle. The result file should be in the following format\n",
    "```\n",
    "id,category\n",
    "S101,0\n",
    "S201,1\n",
    "S102,2\n",
    "...\n",
    "```\n",
    "- Note that you are only allowed to upload **5 copies** of your results to Kaggle per day. Make every upload count, and don't waste your opportunities!\n",
    "\n",
    "**NB** you need to fill in the cells below with your code. If you fail to provide the code, you will get zero for this question. Your code should be well documented and provide methods to generate the prediction files and compute accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ernes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for Question Classification\n",
    "# Created by Umanga Bista.\n",
    "#\n",
    "\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "import json \n",
    "from fasttext import Dataset, map_token_seq_to_word_id_seq, train_fast_text\n",
    "\n",
    "\n",
    "## Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# stopwards\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "# but keep the following words because they are essential for question classification\n",
    "stopwords_en.remove('how')\n",
    "stopwords_en.remove('what')\n",
    "stopwords_en.remove('when')\n",
    "stopwords_en.remove('where')\n",
    "stopwords_en.remove('which')\n",
    "stopwords_en.remove('who')\n",
    "stopwords_en.remove('why')\n",
    "stopwords_en.remove('for')\n",
    "\n",
    "# tokenizer\n",
    "__tokenization_pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "tokenizer = nltk.tokenize.regexp.RegexpTokenizer(__tokenization_pattern)\n",
    "\n",
    "def preprocessor(text):\n",
    "    '''\n",
    "        turns text into tokens after tokenization, stemming, stop words removal\n",
    "        imput:\n",
    "            - text: document to process\n",
    "        output: =>\n",
    "            - tokens: list of tokens after tokenization, stemming, stop words removal\n",
    "    '''\n",
    "    stems = []\n",
    "    tokens = tokenizer.tokenize(text.lower()) # convert to lower case then tokenize\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stopwords_en:   # only keep words and remove stopwords\n",
    "            stems.append(str(stemmer.stem(token)))  # stemming\n",
    "\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    \"\"\"Define the computation graph for fasttext model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim, learning_rate):\n",
    "        \"\"\"Init the model with default parameters/hyperparameters.\"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_func = F.cross_entropy\n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.W = torch.nn.Linear(embedding_dim, num_classes) \n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x, sens_lengths):\n",
    "        x = self.embedder(x).sum(axis=1).div(sens_lengths.float())\n",
    "        x = self.W(x.float())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : train loss = 1.1256445998836668 , validation accuracy = 0.6790000200271606 .\n",
      "Epoch 1 : train loss = 0.8020420275185437 , validation accuracy = 0.7419999837875366 .\n",
      "Epoch 2 : train loss = 0.5884327219513755 , validation accuracy = 0.7919999957084656 .\n",
      "Epoch 3 : train loss = 0.42022691973296034 , validation accuracy = 0.800000011920929 .\n",
      "Epoch 4 : train loss = 0.3004579021755333 , validation accuracy = 0.8399999737739563 .\n",
      "Epoch 5 : train loss = 0.2052320953397054 , validation accuracy = 0.8700000047683716 .\n",
      "Epoch 6 : train loss = 0.14104202358836335 , validation accuracy = 0.8859999775886536 .\n",
      "Epoch 7 : train loss = 0.09610619175548901 , validation accuracy = 0.8949999809265137 .\n",
      "Epoch 8 : train loss = 0.06583537119604932 , validation accuracy = 0.9070000052452087 .\n",
      "Epoch 9 : train loss = 0.04969762310307116 , validation accuracy = 0.9120000004768372 .\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your tasks are to\n",
    "    1. Read in the .json files and create Dataset objects from them. The dataset constructor requires two parameters: a list of\n",
    "        sentences (where each sentence is a list of word ids) and a list of labels (or None is there are no labels).\n",
    "        You will need to apply appropriate preprocessing to the raw text to get in the appropriate form.\n",
    "    2. Run the train_fast_text() function on these Datasets and your model.\n",
    "    3. Convert the output file of predictions into the correct format for Kaggle. \n",
    "        Kaggle expects a csv with two columns, id and category. You need to have these two column headers as the first row.\n",
    "        Your csv should not include any whitespace.\n",
    "    4. Change the model hyper parameters, training settings, text preprocessing, or anything else you see fit\n",
    "        in order to improve your models performance.\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 6\n",
    "learning_rate = 0.5\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "embedding_dim = 300\n",
    "\n",
    "\n",
    "\n",
    "# Read the .json files \n",
    "\n",
    "with open (os.path.join('question_2-2_data', 'train.json'), mode='r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open (os.path.join('question_2-2_data', 'validation.json'), mode='r') as f:\n",
    "    validation_data = json.load(f)\n",
    "with open (os.path.join('question_2-2_data', 'test.json'), mode='r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "# Build vocabulary (word_to_id) from training data\n",
    "vocab = []\n",
    "\n",
    "for s in train_data:\n",
    "    tokens = preprocessor(s['Sentence'])\n",
    "    vocab.extend(tokens)\n",
    "    \n",
    "pad_word_id = 0\n",
    "unknown_word_id = 1\n",
    "count = [['$PAD$', pad_word_id], ['$UNK$', unknown_word_id]]\n",
    "sorted_counts = collections.Counter(vocab).most_common()\n",
    "count.extend(sorted_counts)\n",
    "word_to_id = dict()\n",
    "for word, _ in count:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "    \n",
    "def dataset_from_json(json_data, word_to_id):\n",
    "    \"\"\" Create a labeled Dataset using a dictionary.\n",
    "\n",
    "        Args:\n",
    "            json_data (list) : the data in json format\n",
    "            word_to_id (dictionary) : a dictionary mapping words to their ids.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    data_labels = []\n",
    "    data_tokens = []\n",
    "    for s in json_data:\n",
    "        tokens = preprocessor(s['Sentence'])  # preprocessing on the text\n",
    "        word_id_seq = map_token_seq_to_word_id_seq(tokens, word_to_id)\n",
    "        if len(word_id_seq) > 0:\n",
    "            data_tokens.append(tokens)\n",
    "            data.append(word_id_seq)\n",
    "            if 'Label' in s:\n",
    "                data_labels.append(s['Label'])\n",
    "        else:\n",
    "            print(s['Sentence'])\n",
    "            print(tokens)\n",
    "            \n",
    "    if len(data_labels) == 0:\n",
    "        return Dataset(data, None), data_tokens\n",
    "    return Dataset(data, data_labels), data_tokens\n",
    "   \n",
    "\n",
    "\n",
    "# Construct the Datasets needed\n",
    "train_dataset, train_tokens = dataset_from_json(train_data, word_to_id)\n",
    "valid_dataset, valid_tokens = dataset_from_json(validation_data, word_to_id)\n",
    "test_dataset, test_tokens = dataset_from_json(test_data, word_to_id)\n",
    "\n",
    "model = FastText(len(word_to_id)+2, num_classes, embedding_dim=embedding_dim, learning_rate=learning_rate)\n",
    "\n",
    "model_file_path = os.path.join('models', 'fasttext_model_file_q2-2')\n",
    "train_fast_text(model, train_dataset, valid_dataset, test_dataset, model_file_path, batch_size=batch_size, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print IDs\n",
    "# test_ids = []\n",
    "# for s in test_data:\n",
    "#     test_ids.append(s['ID'])\n",
    "# with open(model_file_path + 'ids.csv', mode='w') as f:\n",
    "#     for r in test_ids:\n",
    "#         f.write(\"%s\\n\" % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparison between Absolute Discounting and Kneser Ney smoothing [2pts]\n",
    "\n",
    "Read the code below for interpolated absolute discounting and implement Kneser Ney smoothing in Python. It is sufficient to assume that the highest order of ngram is two and the discount is 0.75. Evaluate your program on the following ngram corpus and compute the distribution $p(x | \\text{Granny})$ for all possible unigrams in the given corpus. \n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Explain what make the differences regarding the prediction results between interpolated absolute discounting and Kneser Ney smoothing.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_corpus = ['Sam eats apple',\n",
    "          \"Granny plays with Sam\",\n",
    "           \"Sam plays with Smith\",\n",
    "           \"Sam likes Smith\",\n",
    "          \"Sam likes apple\",\n",
    "                \"Sam likes sport\",\n",
    "                \"Sam plays tennis\",\n",
    "                \"Sam likes games\",\n",
    "                \"Sam plays games\",\n",
    "          \"Sam likes apple Granny Smith\"]\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "class NgramStats:\n",
    "    \"\"\" Collect unigram and bigram statistics. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_to_count = Counter([])\n",
    "        self.unigram_to_count = dict()\n",
    "        \n",
    "    def collect_ngram_counts(self, corpus):\n",
    "        \"\"\"Collect unigram and bigram counts from the given corpus.\"\"\"\n",
    "        unigram_counter = Counter([])\n",
    "        for sentence in corpus:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            bigrams = ngrams(tokens, 2)\n",
    "            unigrams = ngrams(tokens, 1)\n",
    "            self.bigram_to_count += Counter(bigrams)\n",
    "            unigram_counter += Counter(unigrams)\n",
    "        self.unigram_to_count = {k[0]:int(v) for k,v in unigram_counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('Sam', 'likes'): 5, ('Sam', 'plays'): 3, ('plays', 'with'): 2, ('likes', 'apple'): 2, ('Sam', 'eats'): 1, ('eats', 'apple'): 1, ('Granny', 'plays'): 1, ('with', 'Sam'): 1, ('with', 'Smith'): 1, ('likes', 'Smith'): 1, ('likes', 'sport'): 1, ('plays', 'tennis'): 1, ('likes', 'games'): 1, ('plays', 'games'): 1, ('apple', 'Granny'): 1, ('Granny', 'Smith'): 1})\n",
      "{'Sam': 10, 'eats': 1, 'apple': 3, 'Granny': 2, 'plays': 4, 'with': 2, 'Smith': 3, 'likes': 5, 'sport': 1, 'tennis': 1, 'games': 2}\n"
     ]
    }
   ],
   "source": [
    "stats = NgramStats()         \n",
    "stats.collect_ngram_counts(ngram_corpus)\n",
    "print(stats.bigram_to_count)\n",
    "print(stats.unigram_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated Absolute Discounting\n",
    "import operator\n",
    "    \n",
    "class AbsDist:\n",
    "    \"\"\"\n",
    "     Implementation of Interpolated Absolute Discounting\n",
    "     \n",
    "     Reference: slide 25 in https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_stats):\n",
    "        \"\"\" Initialization\n",
    "        \n",
    "            Args:\n",
    "                ngram_stats (NgramStats) : ngram statistics.\n",
    "        \"\"\"\n",
    "        self.unigram_freq = float(sum(ngram_stats.unigram_to_count.values()))\n",
    "        self.stats= ngram_stats\n",
    "    \n",
    "    def compute_prop(self, bigram, discount = 0.75):\n",
    "        \"\"\" Compute probability p(y | x)\n",
    "        \n",
    "            Args:\n",
    "                bigram (string tuple) : a bigram (x, y), where x and y denotes an unigram respectively.\n",
    "                discount (float) : the discounter factor for the linear interpolation.\n",
    "        \"\"\"\n",
    "        preceding_word_count = 0\n",
    "        if bigram[0] in self.stats.unigram_to_count:\n",
    "            preceding_word_count = self.stats.unigram_to_count[bigram[0]]\n",
    "            \n",
    "        if preceding_word_count > 0:\n",
    "            left_term = 0\n",
    "            if bigram in self.stats.bigram_to_count:\n",
    "                bigram_count = float(self.stats.bigram_to_count[bigram])\n",
    "                left_term = (bigram_count - discount)/preceding_word_count\n",
    "            right_term = 0\n",
    "            if bigram[1] in self.stats.unigram_to_count:\n",
    "                current_word_count = self.stats.unigram_to_count[bigram[1]]\n",
    "                num_bigram_preceding_word = 0\n",
    "                for c_bigram in self.stats.bigram_to_count.keys():\n",
    "                    if c_bigram[0] == bigram[0] :\n",
    "                        num_bigram_preceding_word += 1\n",
    "                normalization_param = (discount * num_bigram_preceding_word)/ preceding_word_count \n",
    "                p_unigram = current_word_count/self.unigram_freq\n",
    "                right_term = normalization_param * p_unigram\n",
    "            return left_term + right_term\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eats', 0.022058823529411763), ('sport', 0.022058823529411763), ('tennis', 0.022058823529411763), ('Granny', 0.044117647058823525), ('with', 0.044117647058823525), ('games', 0.044117647058823525), ('apple', 0.0661764705882353), ('likes', 0.11029411764705882), ('Smith', 0.19117647058823528), ('plays', 0.21323529411764705), ('Sam', 0.22058823529411764)]\n"
     ]
    }
   ],
   "source": [
    "def compute_prop_abs_dist(ngram_stats, preceding_unigram, d = 0.75):\n",
    "    \"\"\" Compute the distribution p(y | x) of all y given preceding_unigram\n",
    "\n",
    "        Args:\n",
    "            preceding_unigram (string) : the preceding unigram.\n",
    "            d (float) : the discounter factor for the linear interpolation.\n",
    "    \"\"\"\n",
    "    absDist = AbsDist(ngram_stats)\n",
    "    c_unigram_to_prob = dict()\n",
    "    for c_unigram in ngram_stats.unigram_to_count.keys():\n",
    "        if not c_unigram in c_unigram_to_prob:\n",
    "            c_unigram_to_prob[c_unigram] = absDist.compute_prop((preceding_unigram, c_unigram), d)\n",
    "  \n",
    "    sorted_prob = sorted(c_unigram_to_prob.items(), key=operator.itemgetter(1))\n",
    "    return sorted_prob\n",
    "\n",
    "print(compute_prop_abs_dist(stats, 'Granny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eats', 0.022058823529411763), ('sport', 0.022058823529411763), ('tennis', 0.022058823529411763), ('Granny', 0.044117647058823525), ('with', 0.044117647058823525), ('games', 0.044117647058823525), ('apple', 0.0661764705882353), ('likes', 0.11029411764705882), ('Smith', 0.19117647058823528), ('plays', 0.21323529411764705), ('Sam', 0.22058823529411764)]\n"
     ]
    }
   ],
   "source": [
    "def compute_prop_KN(ngram_stats, preceding_word, d=0.75):\n",
    "    # Implement Kneser Ney Smoothing here.\n",
    "    # Hint: try to reuse the above code as much as possible.\n",
    "    absDist = AbsDist(ngram_stats)\n",
    "    c_unigram_to_prob = dict()\n",
    "    for c_unigram in ngram_stats.unigram_to_count.keys():\n",
    "        if not c_unigram in c_unigram_to_prob:\n",
    "            c_unigram_to_prob[c_unigram] = absDist.compute_prop((preceding_word, c_unigram), d)\n",
    "  \n",
    "    sorted_prob = sorted(c_unigram_to_prob.items(), key=operator.itemgetter(1))\n",
    "    return sorted_prob\n",
    "    \n",
    "    \n",
    "\n",
    "print(compute_prop_KN(stats, 'Granny'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "EXPLAIN THE DIFFERENCES REGARDING PREDICTION RESULTS HERE\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kneser-Ney smoothing takes into account the number of different words a unigram follows. That is, the probability of a unigram to follow a new unigram is considered, so that words that only follows one frequent word only will not have a high unigram probability, which would otherwise have a high probability in absolute discounting. Thus, 'Sam' will have a lower probability."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
